{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Data Analytics with PySpark - SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and PySpark Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName('pySparkExamples') \\\n",
    "    .setMaster('local')\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent.as_posix()\n",
    "csv_file = f\"{PROJECT_ROOT}/data/emp_details.json\"\n",
    "uri_scheme = \"file://\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  File Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"userId\", StringType(), True),\n",
    "    StructField(\"jobTitle\", StringType(), True),\n",
    "    StructField(\"firstName\", StringType(), True),\n",
    "    StructField(\"lastName\", StringType(), True),\n",
    "    StructField(\"employeeCode\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"phoneNumber\", StringType(), True),\n",
    "    StructField(\"emailAddress\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_path = f\"{uri_scheme}/{csv_file}\" if sys.platform == \"win32\" else f\"{uri_scheme}{csv_file}\"\n",
    "df = spark.read.json(input_path, schema=schema)\n",
    "\n",
    "# Required for executing SQL Queries\n",
    "df.createOrReplaceTempView(\"Employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "|userId|         jobTitle|  firstName|lastName|employeeCode|region|phoneNumber| emailAddress|salary|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "| cpoul|        Developer|Christopher|  Poulin|          E1|    US|     123456|cpoul@amey.me|100000|\n",
      "| drome|        Developer|      David|    Rome|          E1|    US|    1111111|drome@amey.me|110000|\n",
      "| tjohn|        Developer|        Tin| Johnson|          E3|    US|    2222222|tjohn@amey.me|110000|\n",
      "| akolh|        Developer|       Amey|   Kolhe|          E1|    IN|     123456|akolh@amey.me|125000|\n",
      "| hdwye|   Senior Manager|      Helen|   Dwyer|          E5|    US|    2222222|hdwye@amey.me|150000|\n",
      "|  ajha|   Senior Manager|       Amit|     Jha|          E6|    IN|    2222222| ajha@amey.me|160000|\n",
      "| kkaga|   Vice President|   Kristina| Kaganer|          E7|    US|    2222222|kkaga@amey.me|200000|\n",
      "|  ctau| Business Analyst|      Chris|     Tau|          E8|    AE|    2222222| ctau@amey.me|175000|\n",
      "| tkhou|Data Science Lead|       Theo|  Khoury|          E9|    AE|    2222222|tkhou@amey.me|175000|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "#### Statement: Find distinct jobTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         jobTitle|\n",
      "+-----------------+\n",
      "|   Vice President|\n",
      "|        Developer|\n",
      "|Data Science Lead|\n",
      "| Business Analyst|\n",
      "|   Senior Manager|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.select(\"jobTitle\").distinct()\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         jobTitle|\n",
      "+-----------------+\n",
      "|   Vice President|\n",
      "|        Developer|\n",
      "|Data Science Lead|\n",
      "| Business Analyst|\n",
      "|   Senior Manager|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"SELECT distinct jobTitle from Employees\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "#### Statement: Get all employee details where region is US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "|userId|      jobTitle|  firstName|lastName|employeeCode|region|phoneNumber| emailAddress|salary|\n",
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "| cpoul|     Developer|Christopher|  Poulin|          E1|    US|     123456|cpoul@amey.me|100000|\n",
      "| drome|     Developer|      David|    Rome|          E1|    US|    1111111|drome@amey.me|110000|\n",
      "| tjohn|     Developer|        Tin| Johnson|          E3|    US|    2222222|tjohn@amey.me|110000|\n",
      "| hdwye|Senior Manager|      Helen|   Dwyer|          E5|    US|    2222222|hdwye@amey.me|150000|\n",
      "| kkaga|Vice President|   Kristina| Kaganer|          E7|    US|    2222222|kkaga@amey.me|200000|\n",
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.filter(F.col(\"region\")==\"US\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "|userId|      jobTitle|  firstName|lastName|employeeCode|region|phoneNumber| emailAddress|salary|\n",
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "| cpoul|     Developer|Christopher|  Poulin|          E1|    US|     123456|cpoul@amey.me|100000|\n",
      "| drome|     Developer|      David|    Rome|          E1|    US|    1111111|drome@amey.me|110000|\n",
      "| tjohn|     Developer|        Tin| Johnson|          E3|    US|    2222222|tjohn@amey.me|110000|\n",
      "| hdwye|Senior Manager|      Helen|   Dwyer|          E5|    US|    2222222|hdwye@amey.me|150000|\n",
      "| kkaga|Vice President|   Kristina| Kaganer|          E7|    US|    2222222|kkaga@amey.me|200000|\n",
      "+------+--------------+-----------+--------+------------+------+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"SELECT * from Employees WHERE region = 'US'\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "#### Statement: Find min, max and average salaries for each jobTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+----------+\n",
      "|         jobTitle|min_salary|max_salary|avg_salary|\n",
      "+-----------------+----------+----------+----------+\n",
      "|   Vice President|    200000|    200000|  200000.0|\n",
      "|        Developer|    100000|    125000|  111250.0|\n",
      "|Data Science Lead|    175000|    175000|  175000.0|\n",
      "| Business Analyst|    175000|    175000|  175000.0|\n",
      "|   Senior Manager|    150000|    160000|  155000.0|\n",
      "+-----------------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.groupby([\"jobTitle\"]) \\\n",
    "    .agg(\n",
    "        F.min(\"salary\").alias(\"min_salary\"), \n",
    "        F.max(\"salary\").alias(\"max_salary\"), \n",
    "        F.avg(\"salary\").alias(\"avg_salary\")\n",
    ")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+-----------+\n",
      "|         jobTitle|min(salary)|max(salary)|avg(salary)|\n",
      "+-----------------+-----------+-----------+-----------+\n",
      "|   Vice President|     200000|     200000|   200000.0|\n",
      "|        Developer|     100000|     125000|   111250.0|\n",
      "|Data Science Lead|     175000|     175000|   175000.0|\n",
      "| Business Analyst|     175000|     175000|   175000.0|\n",
      "|   Senior Manager|     150000|     160000|   155000.0|\n",
      "+-----------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"SELECT jobTitle, min(salary), max(salary), avg(salary) from Employees GROUP BY jobTitle\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "#### Statement: Increase Salaries of Developers by 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------------------+\n",
      "|userId|         jobTitle|  firstName|lastName|employeeCode|region|phoneNumber| emailAddress|            salary|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------------------+\n",
      "| cpoul|        Developer|Christopher|  Poulin|          E1|    US|     123456|cpoul@amey.me|110000.00000000001|\n",
      "| drome|        Developer|      David|    Rome|          E1|    US|    1111111|drome@amey.me|121000.00000000001|\n",
      "| tjohn|        Developer|        Tin| Johnson|          E3|    US|    2222222|tjohn@amey.me|121000.00000000001|\n",
      "| akolh|        Developer|       Amey|   Kolhe|          E1|    IN|     123456|akolh@amey.me|          137500.0|\n",
      "| hdwye|   Senior Manager|      Helen|   Dwyer|          E5|    US|    2222222|hdwye@amey.me|          150000.0|\n",
      "|  ajha|   Senior Manager|       Amit|     Jha|          E6|    IN|    2222222| ajha@amey.me|          160000.0|\n",
      "| kkaga|   Vice President|   Kristina| Kaganer|          E7|    US|    2222222|kkaga@amey.me|          200000.0|\n",
      "|  ctau| Business Analyst|      Chris|     Tau|          E8|    AE|    2222222| ctau@amey.me|          175000.0|\n",
      "| tkhou|Data Science Lead|       Theo|  Khoury|          E9|    AE|    2222222|tkhou@amey.me|          175000.0|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.withColumn(\"salary\", F.when(F.col(\"jobTitle\") == \"Developer\", F.col(\"salary\")* 1.10).otherwise(F.col(\"salary\")))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+--------+\n",
      "|userId|         jobTitle|  firstName|lastName|employeeCode|region|phoneNumber| emailAddress|  salary|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+--------+\n",
      "| cpoul|        Developer|Christopher|  Poulin|          E1|    US|     123456|cpoul@amey.me|110000.0|\n",
      "| drome|        Developer|      David|    Rome|          E1|    US|    1111111|drome@amey.me|121000.0|\n",
      "| tjohn|        Developer|        Tin| Johnson|          E3|    US|    2222222|tjohn@amey.me|121000.0|\n",
      "| akolh|        Developer|       Amey|   Kolhe|          E1|    IN|     123456|akolh@amey.me|137500.0|\n",
      "| hdwye|   Senior Manager|      Helen|   Dwyer|          E5|    US|    2222222|hdwye@amey.me|165000.0|\n",
      "|  ajha|   Senior Manager|       Amit|     Jha|          E6|    IN|    2222222| ajha@amey.me|176000.0|\n",
      "| kkaga|   Vice President|   Kristina| Kaganer|          E7|    US|    2222222|kkaga@amey.me|220000.0|\n",
      "|  ctau| Business Analyst|      Chris|     Tau|          E8|    AE|    2222222| ctau@amey.me|192500.0|\n",
      "| tkhou|Data Science Lead|       Theo|  Khoury|          E9|    AE|    2222222|tkhou@amey.me|192500.0|\n",
      "+------+-----------------+-----------+--------+------------+------+-----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql(\"SELECT userId, jobTitle, firstName, lastName, employeeCode, region, phoneNumber, emailAddress, salary * 1.1 as salary from Employees\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# Data Analystics with PySpark - SQL Queries"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}